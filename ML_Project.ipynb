{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKKIkKVEBYgh"
      },
      "source": [
        "# ML Project - Fall 2021  \n",
        "---\n",
        "Javad Hezare  \n",
        "Ali Abbasi  \n",
        "---\n",
        "In this project, we are going to predict customers' behavior when clicking on an advertisement; whether they buy the product or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWPY5azWpQxo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlA7ZhdQOFve"
      },
      "source": [
        "Importing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CWnXZlXFpZua",
        "outputId": "79e9c024-2432-46e0-f5c6-51e7a8fbf895"
      },
      "outputs": [],
      "source": [
        "# df = pd.read_csv('https://github.com/a80-abbasi/ML_Project/blob/main/train_dataset.csv?raw=true')\n",
        "df = pd.read_csv('train_dataset.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy3IdMoZOFvf"
      },
      "source": [
        "For better results, we seperate train set and validation set from the begining:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2gaGHp1OFvf",
        "outputId": "53502874-44eb-49a6-eea7-704e891aa24d"
      },
      "outputs": [],
      "source": [
        "train_data, val_data = train_test_split(df, test_size=0.2)\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rl8GMqWOFvf"
      },
      "outputs": [],
      "source": [
        "# for convenience\n",
        "data = train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RogynRBL-skv"
      },
      "source": [
        "---\n",
        "# 1. EDA, Data Cleaning and Feature Engineering\n",
        "\n",
        "In this part we are going to get some insights about our data and find out what information each variable gives us and then, we start preparing our data for modeling by cleaning and deciding our policy for missing data. To do so we will follow bellow steps:\n",
        "1. Understanding variables\n",
        "2. Analyzing and Visualizing relationships between variables\n",
        "3. Deciding what we should do with missing data\n",
        "4. Choosing which features of data we are going to use to train our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl9WAunIOFvg"
      },
      "source": [
        "Inspecting datatypes of each feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPqT2QhGigVX",
        "outputId": "b67574ab-335d-4969-b6f5-9bee4afc00f7"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOImuslfOFvg"
      },
      "source": [
        "Inspecting count of each unique value in each cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "guUYtYbrOFvh",
        "outputId": "8ae4e20b-7026-48e9-f5d2-a0e0c8d3c9fe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "for col in data.columns:\n",
        "    display(data[col].value_counts().to_frame())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJlwCpeigVd"
      },
      "source": [
        "_'click_timestamp'_ seem to contain only two dates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZzz70CqFFK4",
        "outputId": "1eb83818-958a-4a84-b63b-d5d77b13ffa9"
      },
      "outputs": [],
      "source": [
        "data['click_timestamp'].apply(lambda x: x.split(' ')[0]).unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ew73dw7igVe"
      },
      "source": [
        "We were right. So we convert this column to 2 columns: one indicating the day and on indicating number of seconds from start of the day as integer values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hr0rivmlMySh"
      },
      "outputs": [],
      "source": [
        "def get_seconds(time_str):\n",
        "  _, hour = time_str.split(' ')\n",
        "  t = list(map(int, hour.split(':')))\n",
        "  return t[0] * 3600 + t[1] * 60 + t[2]\n",
        "\n",
        "def get_day(time_str):\n",
        "  date, _ = time_str.split(' ')\n",
        "  return int(date.split('-')[-1])\n",
        "\n",
        "data = data.merge(data['click_timestamp'].apply(lambda s: pd.Series({'click_day':get_day(s), 'click_second':get_seconds(s)})), left_index=True, right_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "IEsOO_WjigVg",
        "outputId": "73c95331-7386-4ce3-9a3e-b550e57bdbdd"
      },
      "outputs": [],
      "source": [
        "data.drop(columns=['click_timestamp'], errors='ignore', inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTeDejJ5OFvi"
      },
      "source": [
        "Splitting columns to 'numerical' and 'categorical' values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KcST1ybK-4S"
      },
      "outputs": [],
      "source": [
        "target = 'Sale'\n",
        "numerical_cols = ['click_second', 'click_day', 'nb_clicks_1week', 'product_price', 'SalesAmountInEuro', 'time_delay_for_conversion']\n",
        "categorical_cols = list(set(data.columns) - set(numerical_cols) - set([target]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgXqK2moOFvj"
      },
      "source": [
        "Inspecting correlation matrix for numerical data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "702Q4MB4OFvj",
        "outputId": "01c1cf56-048b-4028-b779-d2392e82ef71"
      },
      "outputs": [],
      "source": [
        "data[[target] + numerical_cols].corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCdMzAXMOFvj"
      },
      "source": [
        "Three of columns seem suspicious. _'SalesAmountInEuro'_, _'time_delay_for_conversion'_ and _'product_price'_ have relatively high value of correlation with _'Sale'_ and seem to have valid values whenever _'Sale'_ is one and vice versa. So we'd better examine them more closely and decide wether we should retain them or not. If our doubt is right, we should delete those columns and train our model on other columns. Otherwise, our model would be trained only on these columns and most probably do very terribly on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF25IJYZOFvk"
      },
      "outputs": [],
      "source": [
        "def print_metrics_evaluation(true_y, pred_y, model_name=None):\n",
        "    if model_name is not None:\n",
        "        print(f'{model_name}:')\n",
        "    print(f'''accuracy_score = {accuracy_score(true_y, pred_y)}\n",
        "precision_score = {precision_score(true_y, pred_y)}\n",
        "recall_score = {recall_score(true_y, pred_y)}\n",
        "f1_score = {f1_score(true_y, pred_y)}\n",
        "    ''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IigKJ8xqigVZ",
        "outputId": "2ea7a884-59d5-4eaf-f413-c1dd1b33c0a0"
      },
      "outputs": [],
      "source": [
        "SalesAmountInEuro_predict = data['SalesAmountInEuro'] != -1\n",
        "time_delay_for_conversion_predict = data['time_delay_for_conversion'] != -1\n",
        "product_price_predict = data['product_price'] > 0\n",
        "print_metrics_evaluation(data['Sale'], SalesAmountInEuro_predict, 'SalesAmountInEuro')\n",
        "print_metrics_evaluation(data['Sale'], time_delay_for_conversion_predict, 'time_delay_for_conversion')\n",
        "print_metrics_evaluation(product_price_predict, data['Sale'], 'product_price')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNE4up8pOFvk"
      },
      "source": [
        "So for sure we must drop those columns. They are almost equivalent to label (Sale) and we can't use them in training. But we postpone judgment for _'product_price'_ to a short while later. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9SbxzAvLikB"
      },
      "outputs": [],
      "source": [
        "data = data.drop(columns=['SalesAmountInEuro', 'time_delay_for_conversion'], errors='ignore')\n",
        "\n",
        "def remove_from_list(lst, value):\n",
        "    value = value if isinstance(value, list) else [value]\n",
        "    return list(set(lst).difference(value))\n",
        "\n",
        "numerical_cols = remove_from_list(numerical_cols, ['SalesAmountInEuro', 'time_delay_for_conversion'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbWYIeMsigVi"
      },
      "source": [
        "Describing numerical columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "qdZTrwB9igVi",
        "outputId": "5d852779-0009-411e-c952-c38f7ea2c14a"
      },
      "outputs": [],
      "source": [
        "data[numerical_cols].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D38XH9s7igVj"
      },
      "source": [
        "As we can see the difference between 75% and max in _'nb_clicks_1week'_ is too much so there must exist some outlier records in this column and is _'product_price'_, missing data (in this casee 0) is dominating value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQa6TP29igVj",
        "outputId": "727e1aeb-ddd9-4285-d564-d010dd2dfa4a"
      },
      "outputs": [],
      "source": [
        "data[categorical_cols].nunique(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ToKHwOigVk"
      },
      "source": [
        "As we can see, product_category(7) contains only one value, -1. It means non of our records have product_category(7) and we can drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtxTKp-TigVl"
      },
      "outputs": [],
      "source": [
        "data.drop(columns=['product_category(7)'], errors='ignore', inplace=True)\n",
        "categorical_cols = remove_from_list(categorical_cols,'product_category(7)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut9DeD6oigVl"
      },
      "source": [
        "Describing categorical columns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "T2zIAc-yigVm",
        "outputId": "1d794fd0-eb27-438e-994f-137d411f5b4e"
      },
      "outputs": [],
      "source": [
        "data[categorical_cols].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgSAXaGjigVn"
      },
      "source": [
        "Note that _'product_title'_ has nan values.  \n",
        "We replace all invalid values (-1 and 0 in _'product_price'_) to nan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "HHoYzJVtigVn",
        "outputId": "902588cc-b907-4a0b-c783-1019cdc8d4eb"
      },
      "outputs": [],
      "source": [
        "data = data.replace([-1, '-1'], np.nan).replace({'product_price': 0}, np.nan)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_v59xwOFvn"
      },
      "source": [
        "Examining count of not null values in each column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "kELcJ0yYOFvn",
        "outputId": "6e7e6ca4-1443-40fc-fa3a-b96c61890623"
      },
      "outputs": [],
      "source": [
        "data.count().to_frame().set_axis(['count'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u42GM7LxigVo"
      },
      "outputs": [],
      "source": [
        "# todo: converting values with low value_counts to 'other'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxpwqBueigVo"
      },
      "source": [
        "Now we map categorical values to integer values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLlpHaidigVp"
      },
      "outputs": [],
      "source": [
        "data[categorical_cols] = data[categorical_cols].astype('category')\n",
        "column_categories = {}\n",
        "for col in categorical_cols:\n",
        "    column_categories[col] = data[col].cat.categories\n",
        "    data[col] = data[col].cat.codes\n",
        "# data[categorical_cols] = data[categorical_cols].astype('Int64')\n",
        "data = data.replace(-1, np.nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBUDpsDAigVp"
      },
      "source": [
        "Plotting boxplot for numerical values in order to find outliers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "kTR2W8_3igVq",
        "outputId": "f64c9b71-8f01-4593-8355-2c014a98fd4d"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "for col, ax in zip(numerical_cols, axes.flatten()):\n",
        "    ax.set_title(col)\n",
        "    sns.boxplot(data=data, y=col, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY79G2n7igVq"
      },
      "source": [
        "Boxplots suggest a very small range for non-outlier values for nb_clicks_1week and product_price and if we remove outliers according to them, almost all of their value will be zero and we gain no information by using these features any more. So we choose to use 10000 and 2000 for their upper limits respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U65LULphigVq"
      },
      "outputs": [],
      "source": [
        "nb_click_threshold = 10000\n",
        "product_price_threshold = 2000\n",
        "data.loc[(data['nb_clicks_1week'] > nb_click_threshold), 'nb_clicks_1week'] = np.nan\n",
        "data.loc[(data['product_price'] > product_price_threshold), 'product_price'] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE3KcpYzigVr"
      },
      "source": [
        "Next, we drop rows and columns with too much NaNs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lIio4fOigVr",
        "outputId": "7769111b-2bd3-4a18-916b-21c4ff5f3473"
      },
      "outputs": [],
      "source": [
        "col_threshold = 0.8\n",
        "#Dropping categorical columns with missing value rate higher than threshold\n",
        "dropping_columns = data[categorical_cols].columns[data[categorical_cols].isnull().mean() >= col_threshold]\n",
        "categorical_cols = list(set(categorical_cols).difference(list(dropping_columns)))\n",
        "data.drop(columns=dropping_columns, errors='ignore', inplace=True)\n",
        "\n",
        "row_threshold = 0.6\n",
        "# #Dropping rows with missing value rate higher than threshold\n",
        "data = data.loc[data.isnull().mean(axis=1) < row_threshold]\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGY8PR7rigVs"
      },
      "source": [
        "So this process droped $2$ columns and about $\\frac{1}{4}$ of rows.  \n",
        "When it comes to dealing with missing values, we have three options for replacing NaN values in categorical columns.  \n",
        "1. Replacing them with value that have maximum frequency in that column.  \n",
        "2. Replacing them with median of their columns.\n",
        "3. Introducing a new category, i.e., 'other' for them.  \n",
        "\n",
        "\n",
        "For second option, there should be an order between values and here we don't have any order (even in ordinal features like product_age_group, we can't deduce their order because of hashed strings). And because of very high missing rate in our dataset, first option doesn't seem very appropriate too.  \n",
        "Hence, we are going to use a combination of option 1 and 3. We will set NaN values to value that have maximum frequency in that column and create another column which indicates if this is value was NaN initailly or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "P0HuxFSgigVs",
        "outputId": "b2b27469-fe5b-489a-a236-8aea983a8ddd"
      },
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "clean_df = data[numerical_cols]\n",
        "numerical_cols_medians = clean_df.median()\n",
        "clean_df.fillna(numerical_cols_medians, inplace=True)\n",
        "categorical_cols_idmaxes = {}\n",
        "for col in categorical_cols:\n",
        "    idxmax_value = data[col].mode()[0]\n",
        "    categorical_cols_idmaxes[col] = idxmax_value\n",
        "    clean_df[col] = data[col]\n",
        "    clean_df[f'{col}_is_na'] = clean_df[col].isna() * 1\n",
        "    clean_df[col].fillna(idxmax_value, inplace=True)\n",
        "clean_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFumLlAOFvp"
      },
      "source": [
        "Normalizing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ex7nUYbFigVt",
        "outputId": "f68f0493-e129-4297-e133-315abdac07cf"
      },
      "outputs": [],
      "source": [
        "saved_mean = clean_df.mean()\n",
        "saved_std = clean_df.std()\n",
        "clean_df = (clean_df - saved_mean) / saved_std\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VAxYZwMOFvp"
      },
      "source": [
        "Some of 'is_na' columns had zero variance (i.e., only one unique value) and after normalization they became NaN. So we can drop them without loosing any information:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "mNwoGyBWigVt",
        "outputId": "9568f6b6-d5a5-4302-9943-2f8afefb12cc"
      },
      "outputs": [],
      "source": [
        "nan_is_nan_cols = clean_df.columns[saved_std == 0]\n",
        "clean_df.drop(columns=nan_is_nan_cols, errors='ignore', inplace=True)\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tHtcdCSOFvp"
      },
      "source": [
        "Generating final correlation matrix and its heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F7XxdcPfOFvp",
        "outputId": "c6986a6b-a98f-4599-ffb1-4c70fded33ce"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15,15))\n",
        "correlation_matrix = pd.concat([data[target], clean_df], axis=1).corr()\n",
        "sns.heatmap(correlation_matrix)\n",
        "correlation_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ei4BwX5OFvp"
      },
      "source": [
        "We trained our model without dropping '_product_price_' once, and as we had predicted, we got f1 score almost equal to 1! Which is irrational and this time we we'll try training our model without this column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-ySPJIkOFvq"
      },
      "outputs": [],
      "source": [
        "clean_df.drop(columns=['product_price'], errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFN2Un7NOFvq"
      },
      "source": [
        "For more convenience, we define a method that gets a dataframe and performs all above steps on it. This function will be helpfull for validating our validation (and of course, test) set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcS0QJ7zigVu"
      },
      "outputs": [],
      "source": [
        "def data_preprocess(data, dropping_columns, numerical_median, categories_mode, saved_mean, saved_std, ce):\n",
        "\n",
        "    # convert 'click_timestamp' to ['click_day', 'click_second']\n",
        "    data['click_day'] = data['click_timestamp'].apply(get_day)\n",
        "    data['click_second'] = data['click_timestamp'].apply(get_seconds)\n",
        "    data.drop(columns=['click_timestamp'], errors='ignore', inplace=True)\n",
        "\n",
        "    # drop columns\n",
        "    data = data.drop(columns=dropping_columns, errors='ignore')\n",
        "\n",
        "    # replace missing values and outliers with nan\n",
        "    data = data.replace([-1, '-1'], np.nan).replace({'product_price': 0}, np.nan)\n",
        "    data.loc[(data['nb_clicks_1week'] > nb_click_threshold), 'nb_clicks_1week'] = np.nan\n",
        "    # data.loc[(data['product_price'] > product_price_threshold), 'product_price'] = np.nan\n",
        "    \n",
        "    # now lets fill nan\n",
        "    target = 'Sale'\n",
        "    numerical_cols = data.select_dtypes(include='number').drop(columns=[target, 'click_day', 'click_second']).columns.tolist()\n",
        "    categorical_cols = data.select_dtypes(exclude='number').columns.tolist()\n",
        "    \n",
        "    clean_df = data.drop(columns=[target])\n",
        "    \n",
        "    # fill nan of numerical columns\n",
        "    clean_df[numerical_cols] = clean_df[numerical_cols].fillna(numerical_median)\n",
        "    \n",
        "    # fill nan of categorical columns\n",
        "    for col in categorical_cols:\n",
        "        clean_df[f'{col}_is_na'] = data[col].isna() * 1\n",
        "        clean_df[col].fillna(categories_mode[col], inplace=True)\n",
        "\n",
        "    # encode categories\n",
        "    clean_df[categorical_cols] = ce.transform(data[categorical_cols])\n",
        "        \n",
        "    # normalize numerical features\n",
        "    clean_df[numerical_cols] = (clean_df[numerical_cols] - saved_mean) / saved_std\n",
        "    \n",
        "    # add target column to dataframe\n",
        "    clean_df[target] = data[target]\n",
        "\n",
        "    return clean_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ4MeM9bigVu"
      },
      "source": [
        "# 2. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpSyM-wQOFvq"
      },
      "source": [
        "Now we will define different models and train them with our training set and evaluate our model with validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRhQgd5MOFvq"
      },
      "source": [
        "## 2.1 Neural Network Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEt0RmmgOFvr"
      },
      "source": [
        "We will define and train a neural network model using pytorch as we learned during the course and evaluate metrics (especially $F1$ score) on it.  \n",
        "We will track all of our hyperparameters, metric values, learning curves and ... with mlflow in every run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEY83F_2igVu"
      },
      "outputs": [],
      "source": [
        "# !pip install mlflow\n",
        "# import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import pickle\n",
        "\n",
        "from typing import Tuple\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idzbkWnNigVv",
        "outputId": "1d8f7920-2f37-49b3-b93d-bae1b72acb22"
      },
      "outputs": [],
      "source": [
        "# for using GPU if possible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EcPrU5MOFvr"
      },
      "source": [
        "We define a simple neural network model consisting multiple Linear layers, ReLU activation functions and Dropout for preventing overfitting on training data.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmeM2kzJigVw"
      },
      "outputs": [],
      "source": [
        "class SalePrediction(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        #input\n",
        "        self.input_size = input_size\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.lnn = nn.Sequential(\n",
        "            nn.Linear(self.input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "        \n",
        "    \n",
        "    def forward(self, x: torch.tensor):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        return self.lnn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY840i-yOFvr"
      },
      "source": [
        "Defining a simple Dataset class for our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxsLpJmYigVw"
      },
      "outputs": [],
      "source": [
        "class SalePredictionDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, Y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.Y = torch.from_numpy(Y)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, i: int) -> Tuple[torch.Tensor, ...]:\n",
        "        return self.X[i], self.Y[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doRocVRcOFvs"
      },
      "source": [
        "Creating Dataset and Dataloader from our training and validation data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34NQ5behigVw"
      },
      "outputs": [],
      "source": [
        "Y_train = data[target].to_numpy(dtype='float32')\n",
        "X_train = clean_df.to_numpy(dtype='float32')\n",
        "\n",
        "train_set = SalePredictionDataset(X_train, Y_train)\n",
        "# train_size = int(0.8 * len(data_set))\n",
        "# val_size = len(data_set) - train_size\n",
        "# train_set, val_set = torch.utils.data.random_split(data_set, (train_size, val_size))\n",
        "processed_val_data = data_preprocess(val_data)\n",
        "Y_val = val_data[target].to_numpy(dtype='float32')\n",
        "X_val = processed_val_data.to_numpy(dtype='float32')\n",
        "val_set = SalePredictionDataset(X_val, Y_val)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW6-bnEkigVx"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, epoch):\n",
        "    train_loss = 0\n",
        "    N_train = len(train_loader.dataset)\n",
        "\n",
        "    model.train()\n",
        "    with tqdm.tqdm(enumerate(train_loader), total=len(train_loader)) as pbar:\n",
        "        for i, (x, y) in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.view(-1, 1).to(device)\n",
        "            p = model(x)\n",
        "\n",
        "            loss = criterion(p, y)\n",
        "            train_loss += loss.item() * len(x)\n",
        "\n",
        "            pbar.set_description(f'Epoch:{epoch}, Train Loss: {train_loss / N_train:.3e}')\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    train_loss /= N_train\n",
        "    mlflow.log_metric('train_loss', train_loss)\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "def validate(model, criterion, epoch):\n",
        "    val_loss = 0\n",
        "    N_val = len(val_loader.dataset)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad(), tqdm.tqdm(enumerate(val_loader), total=len(val_loader)) as pbar:\n",
        "        for i, (x, y) in pbar:\n",
        "            x = x.to(device)\n",
        "            y = y.view(-1, 1).to(device)\n",
        "\n",
        "            p = model(x)\n",
        "            loss = criterion(p, y)\n",
        "            val_loss += loss.item() * len(x)\n",
        "\n",
        "            pbar.set_description(f'Epoch:{epoch}, Val Loss: {val_loss / N_val:.3e}')\n",
        "    \n",
        "    print('-------------------------------------------------------------------')\n",
        "    val_loss /= N_val\n",
        "    mlflow.log_metric('val_loss', val_loss)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX6OALlqOFvs"
      },
      "source": [
        "Training model, tracking and logging hyperparameters and artifacts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jwmZQYsKHN3"
      },
      "outputs": [],
      "source": [
        "def do_expriment(lr, num_epochs):\n",
        "    model = SalePrediction(clean_df.shape[1]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    train_loss_arr, val_loss_arr = np.zeros(num_epochs), np.zeros(num_epochs)\n",
        "\n",
        "    with mlflow.start_run():\n",
        "        mlflow.log_param('learning_rate', lr)\n",
        "        mlflow.log_param('num_epochs', num_epochs)\n",
        "        \n",
        "        val_loss_min = float('inf')\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = train(model, criterion, optimizer, epoch)\n",
        "            val_loss = validate(model, criterion, epoch)\n",
        "\n",
        "            train_loss_arr[epoch] = train_loss\n",
        "            val_loss_arr[epoch] = val_loss\n",
        "\n",
        "            if val_loss <= val_loss_min:\n",
        "                torch.save(model.state_dict(), 'NNModel1.pt')\n",
        "                val_loss_min = val_loss\n",
        "\n",
        "        # load best model during different epochs\n",
        "        model.load_state_dict(torch.load('NNModel1.pt'))\n",
        "\n",
        "        # log trained model\n",
        "        print(\"\\nLogging the trained model as a run artifact...\")\n",
        "        mlflow.pytorch.log_model(model, artifact_path=\"pytorch-model\", pickle_module=pickle)\n",
        "        print('Logging the trained model is done')\n",
        "\n",
        "        # metrics\n",
        "        X_train, Y_train = train_set[:]\n",
        "        X_val, Y_val = val_set[:]\n",
        "\n",
        "        train_preds = model(X_train.to(device)).view(-1) >= 0\n",
        "        val_preds = model(X_val.to(device)).view(-1) >= 0\n",
        "\n",
        "        mlflow.log_metric('Train Precision', precision_score(Y_train, train_preds.cpu(), average='macro'))\n",
        "        mlflow.log_metric('Train Recall' ,recall_score(Y_train, train_preds.cpu(), average='macro'))\n",
        "        mlflow.log_metric('Train F1Score', f1_score(Y_train, train_preds.cpu()))\n",
        "\n",
        "        mlflow.log_metric('val Precision', precision_score(Y_val, val_preds.cpu(), average='macro'))\n",
        "        mlflow.log_metric('val Recall', recall_score(Y_val, val_preds.cpu(), average='macro'))\n",
        "        mlflow.log_metric('val F1Score', f1_score(Y_val, val_preds.cpu()))\n",
        "\n",
        "        return train_loss_arr, val_loss_arr, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIdFiSvORVsk",
        "outputId": "7a4d098b-1cba-4d67-f8e5-2f282e38213f"
      },
      "outputs": [],
      "source": [
        "train_loss_arr, val_loss_arr, model = do_expriment(lr=1e-3, num_epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZh_LL4PRs8I"
      },
      "outputs": [],
      "source": [
        "# !zip -r mlruns.zip ./mlruns/\n",
        "# !rm -rf mlruns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSkCs7AhigVy"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_loss_arr, label='train')\n",
        "plt.plot(val_loss_arr, label='val')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEu-A5AeuYxW"
      },
      "outputs": [],
      "source": [
        "# load the best model during epochs according to validation loss\n",
        "model.load_state_dict(torch.load('NNModel1.pt'))\n",
        "model = model.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHNEUwbfumTd"
      },
      "outputs": [],
      "source": [
        "print_metrics_evaluation(Y_train.view(-1).detach(), model(X_train).view(-1).detach() >= 0, 'Metrics on Training Data')\n",
        "print_metrics_evaluation(Y_val.view(-1).detach(), model(X_val).view(-1).detach() >= 0, 'Metrics on Validation Data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4NEcKqj9YhN"
      },
      "source": [
        "## 2.2 XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdMgCf3n9hjY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ds_7kSe-BTzQ"
      },
      "outputs": [],
      "source": [
        "Y = data[target].to_numpy(dtype='float32')\n",
        "X = clean_df.drop(columns=['product_price']).to_numpy(dtype='float32')\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
        "D_val = xgb.DMatrix(X_val, label=Y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ogkyWPo-0Th"
      },
      "outputs": [],
      "source": [
        "param = {\n",
        "    'eta': 0.3,\n",
        "    'max_depth': 4,\n",
        "    'objective': 'binary:logitraw'\n",
        "}\n",
        "\n",
        "step = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udoMKWlHBGbO"
      },
      "outputs": [],
      "source": [
        "model = xgb.train(param, D_train, step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYLwvNvHBPc9"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(D_val)\n",
        "preds = preds > 0\n",
        "\n",
        "# best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "\n",
        "print(\"Precision = {}\".format(precision_score(Y_val, preds, average='macro')))\n",
        "print(\"Recall = {}\".format(recall_score(Y_val, preds, average='macro')))\n",
        "print(\"Accuracy = {}\".format(f1_score(Y_val, preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class Preproccess(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "        self.nb_click_threshold = 10000\n",
        "        self.product_price_threshold = 2000\n",
        "        self.col_threshold = 0.8\n",
        "        self.row_threshold = 0.6\n",
        "\n",
        "    def fit(self, df, y=None):\n",
        "        print('-----[start fitting preproccessore transformer to data]-----')\n",
        "        \n",
        "        # replace missing values with nan\n",
        "        df = df.replace([-1, '-1'], np.nan).replace({'product_price': 0}, np.nan)\n",
        "        df.loc[(df['nb_clicks_1week'] > self.nb_click_threshold), 'nb_clicks_1week'] = np.nan\n",
        "        # df.loc[(df['product_price'] > self.product_price_threshold), 'product_price'] = np.nan\n",
        "\n",
        "        # store those columns we want to drop\n",
        "        categorical_cols = df.select_dtypes(exclude='number').columns.tolist()\n",
        "        dropping_columns = df[categorical_cols].columns[df[categorical_cols].isnull().mean() >= self.col_threshold].tolist()\n",
        "        dropping_columns += (['SalesAmountInEuro', 'time_delay_for_conversion', 'click_timestamp', 'product_category(7)', 'product_price'])\n",
        "\n",
        "        # drop rows from df\n",
        "        df = df.loc[df.isnull().mean(axis=1) < self.row_threshold]\n",
        "\n",
        "        # copy df to store mean, mode, std, and ... of data to use in transform()\n",
        "        copy_df = df.drop(columns=dropping_columns)\n",
        "\n",
        "        target = 'Sale'\n",
        "        numerical_cols = copy_df.select_dtypes(include='number').drop(columns=[target]).columns.tolist()\n",
        "        categorical_cols = copy_df.select_dtypes(exclude='number').columns.tolist()\n",
        "\n",
        "        # numerical columns median and categorical columns mode\n",
        "        numerical_median = copy_df[numerical_cols].median()\n",
        "        categories_mode = dict(copy_df[categorical_cols].mode().loc[0])\n",
        "\n",
        "        # save a TargetEncoder for encoding categories\n",
        "        ce = MyTargetEncoder(target, categorical_cols)\n",
        "        copy_df[categorical_cols] = ce.fit_transform(copy_df)\n",
        "        \n",
        "        # save mean and std of columns\n",
        "        saved_mean = copy_df[numerical_cols].mean()\n",
        "        saved_std = copy_df[numerical_cols].std()\n",
        "\n",
        "        # drop those columns with std = 0\n",
        "        dropping_columns += copy_df[numerical_cols].columns[saved_std == 0].tolist()\n",
        "\n",
        "\n",
        "        # saving parameters to use in transform\n",
        "        self.ce = ce\n",
        "        self.dropping_columns = dropping_columns\n",
        "        self.numerical_median = numerical_median\n",
        "        self.categories_mode = categories_mode\n",
        "        self.saved_mean = saved_mean\n",
        "        self.saved_std = saved_std\n",
        "        \n",
        "        print('-----[fitting transformer to data is done!]-----')\n",
        "        return self\n",
        "\n",
        "    def transform(self, df, y=None):\n",
        "        return self.func(df, self.dropping_columns, self.numerical_median, self.categories_mode, self.saved_mean, self.saved_std, self.ce)\n",
        "\n",
        "\n",
        "class MyTargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, target, columns):\n",
        "        self.columns = columns\n",
        "        self.target = target\n",
        "        self.encoders = {}\n",
        "\n",
        "    def fit(self, df, y=None):\n",
        "        for col in self.columns:\n",
        "            ce = TargetEncoder()\n",
        "            ce.fit(df[col], df[self.target])\n",
        "\n",
        "            self.encoders[col] = ce\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def transform(self, df, y=None):\n",
        "        for col in self.columns:\n",
        "            df[col] = self.encoders[col].transform(df[col])\n",
        "        return df[self.columns]\n",
        "\n",
        "\n",
        "class Model(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, nn_type, target):\n",
        "        self.nn_type = nn_type\n",
        "        self.target = target\n",
        "    \n",
        "    def fit(self, df, y=None):\n",
        "        _, _, self.model = do_experiment(self.nn_type, df, lr=1e-4, num_epochs=2)\n",
        "        return self\n",
        "\n",
        "    def transform(self, df, y=None):\n",
        "        X = torch.from_numpy(df.drop(columns=[target]).to_numpy(dtype=np.float32)).to(device)\n",
        "        return self.model(X)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = pd.read_csv('train_dataset.csv')\n",
        "\n",
        "ml_pipeline = Pipeline([\n",
        "    ('preproccess', Preproccess(data_preprocess)),\n",
        "    ('model', Model(SalePrediction, 'Sale'))\n",
        "])\n",
        "\n",
        "ml_pipeline.fit_transform(data)\n",
        "# ml_pipeline.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mlflow models build-docker \\\n",
        "  -m \"C:\\Users\\LEGION\\Desktop\\MLproj\\testing\\mlruns\\0\\cc7b702a14a240f69cdcbbb0d25fa8c4\\artifacts\\pytorch-model\" \\\n",
        "  -n \"my-docker-image\" \\\n",
        "  --enable-mlserver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_arr, val_arr, model = do_experiment(lr=1e-4, num_epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "S4NEcKqj9YhN"
      ],
      "name": "ML_Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
